{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf0a01ff",
   "metadata": {},
   "source": [
    "### Package import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6babb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import onnxruntime\n",
    "from typing import List, Dict, Any\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30e2896",
   "metadata": {},
   "source": [
    "### Ground truth Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04c9306",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = pd.read_csv(\"../dataset/milk_true.csv\")\n",
    "\n",
    "mapping = {\n",
    "    (1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0): 0,\n",
    "    (0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0): 1,\n",
    "    (0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0): 2,\n",
    "    (0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0): 3,\n",
    "    (0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0): 4,\n",
    "    (0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0): 5,\n",
    "    (0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0): 6,\n",
    "    (0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0): 7,\n",
    "    (0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0): 8,\n",
    "    (0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0): 9,\n",
    "    (0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0): 10\n",
    "}\n",
    "ground_truth = pd.DataFrame({\n",
    "    'lesion_id':ground_truth[\"lesion_id\"],\n",
    "    'Class': ground_truth.iloc[:, 1:].values.tolist()\n",
    "})\n",
    "ground_truth['Class'] = ground_truth['Class'].apply(tuple).map(mapping)\n",
    "ground_truth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745de697",
   "metadata": {},
   "source": [
    "### Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2831e74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "mapping = {\n",
    "    'head_neck_face': 'Head',\n",
    "    'lower_extremity': 'Leg',\n",
    "    'upper_extremity': 'Arm',\n",
    "    'trunk': 'Torso',\n",
    "    'foot': 'Feet',\n",
    "    'genital': 'Genitalia',\n",
    "    'hand': 'Hand'\n",
    "}\n",
    "dataset = pd.read_csv(\"../dataset/milk_meta.csv\")\n",
    "dataset = dataset[dataset['image_type'] == 'dermoscopic']\n",
    "dataset = dataset[[\"lesion_id\", \"isic_id\", \"age_approx\", \"sex\", \"site\"]]\n",
    "dataset = dataset.dropna(subset=[\"age_approx\", \"site\"])\n",
    "dataset['site'] = dataset['site'].map(mapping)\n",
    "\n",
    "dataset = dataset.rename(columns={\n",
    "    'age_approx': 'Age',\n",
    "    'sex': \"Gender\",\n",
    "    'site': \"Location\",\n",
    "})\n",
    "dataset = dataset.reset_index(drop=True)\n",
    "dataset = pd.merge(dataset, ground_truth, on = \"lesion_id\", how = \"left\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c571288",
   "metadata": {},
   "source": [
    "### Preparing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97eeea9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from enum import Enum, IntEnum\n",
    "\n",
    "class LocationId(IntEnum):\n",
    "    \"\"\"Body location enumeration for skin lesions.\"\"\"\n",
    "    ARM = 1\n",
    "    FEET = 2\n",
    "    GENITALIA = 3\n",
    "    HAND = 4\n",
    "    HEAD = 5\n",
    "    LEG = 6\n",
    "    TORSO = 7\n",
    "\n",
    "def _get_location_value(location_str: str) -> int:\n",
    "    \"\"\"Convert location string to numerical value using LocationId enum.\"\"\"\n",
    "    if not location_str:\n",
    "        return -1\n",
    "    \n",
    "    try:\n",
    "        # Convert to uppercase to match enum names\n",
    "        location_enum = LocationId[location_str.upper()]\n",
    "        return location_enum.value\n",
    "    except KeyError:\n",
    "        # Unknown/invalid location\n",
    "        return -1\n",
    "\n",
    "\n",
    "def _prepare_metadata_array(metadata: list[Dict[str, Any]]):\n",
    "    \"\"\"Convert metadata list to numpy array for ONNX model input\"\"\"\n",
    "    # Convert metadata to numerical format\n",
    "    metadata_array = None\n",
    "    \n",
    "    for entry in metadata:\n",
    "        age = entry.get('age', 0) if entry.get('age') is not None else 0\n",
    "        # Convert gender to numerical: male=1, female=0, unknown=-1\n",
    "        gender_str = entry.get('gender', '').lower() if entry.get('gender') else ''\n",
    "        if gender_str in ['male', 'm']:\n",
    "            gender = 1\n",
    "        elif gender_str in ['female', 'f']:\n",
    "            gender = 0\n",
    "        else:\n",
    "            gender = -1  # Unknown/missing gender\n",
    "        \n",
    "        # Convert location to numerical using LocationId enum\n",
    "        location_str = entry.get('location', '').lower() if entry.get('location') else ''\n",
    "        location = _get_location_value(location_str)\n",
    "        \n",
    "        metadata_array = [age, gender, location]\n",
    "    # return metadata_array\n",
    "    return np.array(metadata_array, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a19912",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = []\n",
    "for i in range(len(dataset)):\n",
    "    data.append([{\"age\":dataset.iloc[i][\"Age\"], \"gender\":dataset.iloc[i][\"Gender\"], \"location\":dataset.iloc[i][\"Location\"]}])\n",
    "\n",
    "\n",
    "data = pd.DataFrame({\"metadata\":data})\n",
    "data = dataset.join(data)\n",
    "metadata = []\n",
    "for i in range(len(data)):\n",
    "    metadata_array = _prepare_metadata_array(data.iloc[i][\"metadata\"])\n",
    "    metadata.append(metadata_array)\n",
    "    \n",
    "data = data[[\"lesion_id\", \"isic_id\", \"Class\"]]\n",
    "data = data.join(pd.DataFrame({\"demographics\":metadata}))\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878bf881",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "6c66e50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "class SkinLesionDataset(Dataset):\n",
    "    def __init__(self, data:pd.DataFrame, transform=None):\n",
    "        self.data_frame = data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(f\"../dataset/masked_images/{self.data_frame.iloc[idx, 1]}.jpg\").convert('RGB')\n",
    "        # Resize to 512x512\n",
    "        image = image.resize((512, 512))\n",
    "        # Convert to numpy array with [0,512] range\n",
    "        image = np.array(image, dtype=np.float32)\n",
    "        # Scale from [0,255] to [0,512]\n",
    "        image = image * (512.0 / 255.0)\n",
    "        \n",
    "        image = np.transpose(image, (2, 0, 1))\n",
    "        demographics = self.data_frame.iloc[idx, 3]  # Assuming demographic data starts from second column\n",
    "        label = self.data_frame.iloc[idx, 2]  # Assuming last column is the label\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, demographics, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "e07126e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, criterion, optimizer, scheduler, num_epochs=10, device='cpu'):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for batch in train_loader:\n",
    "            # Unpack the batch\n",
    "            images, demographics, labels = batch\n",
    "            # Move tensors to the specified device\n",
    "            images = images.to(device)\n",
    "            demographics = demographics.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images, demographics)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "\n",
    "        scheduler.step()\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}')\n",
    "\n",
    "def validate_model(model, val_loader, device='cpu'):\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            # Unpack the batch\n",
    "            images, demographics, labels = batch\n",
    "\n",
    "            # Move tensors to the specified device\n",
    "            images = images.to(device)\n",
    "            demographics = demographics.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images, demographics)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_samples += labels.size(0)\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = total_correct / total_samples\n",
    "    print(f'Validation Accuracy: {accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1e7c64",
   "metadata": {},
   "source": [
    "### Yolo Model for tricorder-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "374ce441",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import torch\n",
    "import torch\n",
    "yolo_model = YOLO(\"yolo11n-cls.pt\")\n",
    "\n",
    "\n",
    "class YoloSkinLesionModel(nn.Module):\n",
    "    def __init__(self, num_classes=11):\n",
    "        super().__init__()\n",
    "        self.features = yolo_model.model\n",
    "\n",
    "    def forward(self, image):\n",
    "        image_features = self.features(image)\n",
    "        logits = image_features[0]  # Assuming the last element contains the class logits\n",
    "        \n",
    "        # probabilities = nn.functional.softmax(logits, dim=1)\n",
    "        return logits\n",
    "\n",
    "model = YoloSkinLesionModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "092afd78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.8790e-05, 2.3751e-04, 2.4194e-04, 6.5015e-05, 3.8219e-04, 6.9760e-03, 2.9629e-03, 1.1742e-05, 3.4069e-05, 6.0960e-05, 2.3738e-04, 1.8362e-06, 1.0785e-04, 6.9470e-05, 3.4482e-06, 9.6169e-05, 5.0513e-06, 1.3797e-06, 4.7612e-05, 4.6992e-06, 1.4593e-05, 1.9119e-04, 3.4659e-05, 5.2969e-04, 2.0363e-05, 1.0978e-05,\n",
       "         1.8526e-03, 6.7083e-04, 6.9065e-05, 3.3632e-03, 1.9830e-06, 1.0705e-05, 6.8715e-05, 1.7962e-05, 1.1758e-04, 4.4054e-05, 6.5479e-05, 5.6125e-06, 1.4071e-03, 4.7281e-06, 2.4888e-06, 4.0246e-04, 3.5702e-05, 8.9308e-06, 2.5310e-05, 7.2532e-05, 4.7138e-06, 2.4647e-05, 4.5391e-06, 4.4514e-06, 8.6625e-06, 1.0337e-06,\n",
       "         4.7355e-04, 4.6870e-05, 2.0072e-04, 4.6409e-07, 2.7110e-06, 8.4682e-07, 1.3717e-04, 2.5602e-05, 1.9311e-04, 4.1580e-06, 3.0408e-04, 1.7678e-05, 3.1534e-05, 1.1181e-03, 2.1899e-04, 2.0960e-06, 2.9929e-04, 4.2355e-05, 1.1229e-04, 4.3304e-04, 4.7007e-05, 2.8546e-03, 1.4672e-04, 2.6341e-03, 2.2597e-04, 4.2415e-04,\n",
       "         2.2998e-01, 5.3524e-03, 1.4045e-03, 3.3381e-04, 6.1272e-05, 2.7545e-04, 8.8419e-06, 7.7100e-05, 1.4727e-04, 4.5809e-06, 5.4959e-06, 2.2983e-05, 3.8980e-06, 3.7488e-05, 3.0431e-04, 5.1350e-06, 2.6386e-05, 1.1024e-05, 4.8806e-06, 7.9806e-07, 1.3123e-05, 5.7974e-05, 9.1146e-06, 7.8557e-05, 1.6105e-04, 2.6112e-04,\n",
       "         1.0365e-04, 1.9393e-07, 1.9217e-04, 2.5000e-03, 5.1046e-04, 7.7008e-05, 1.3991e-04, 1.9248e-02, 9.5998e-04, 3.4856e-04, 7.4523e-04, 1.1241e-03, 6.6161e-05, 9.4878e-05, 1.2869e-05, 1.5121e-05, 1.3216e-05, 9.5681e-06, 7.5908e-05, 4.4465e-06, 2.0534e-04, 3.2832e-04, 3.6541e-02, 3.7818e-05, 3.5803e-04, 2.2753e-04,\n",
       "         1.0292e-04, 1.0275e-05, 9.6498e-06, 6.2684e-05, 3.9249e-05, 3.1245e-05, 1.1912e-06, 8.6181e-06, 4.1272e-04, 9.3235e-07, 1.5049e-06, 6.8118e-06, 4.7211e-06, 7.0829e-06, 3.6655e-06, 4.3174e-06, 8.6211e-05, 2.1430e-04, 1.6255e-05, 9.6315e-04, 1.1547e-04, 1.1647e-04, 1.1424e-03, 2.0231e-04, 6.2847e-04, 9.2750e-05,\n",
       "         1.0987e-03, 1.3724e-04, 1.1685e-05, 3.0056e-05, 1.8544e-04, 9.2276e-05, 3.4489e-05, 8.4971e-05, 3.7542e-05, 2.7314e-05, 5.9065e-05, 5.3699e-05, 2.1276e-04, 7.2301e-04, 1.8511e-05, 4.9495e-05, 1.4615e-04, 2.1820e-04, 6.7337e-06, 6.1309e-05, 2.7181e-05, 1.5019e-05, 1.1990e-04, 1.0983e-05, 1.2267e-05, 6.1907e-05,\n",
       "         4.6620e-06, 5.1227e-06, 1.2391e-04, 9.7174e-05, 7.2914e-05, 1.6851e-05, 3.8107e-05, 8.8770e-06, 1.0672e-04, 4.3153e-05, 4.2642e-05, 1.3146e-05, 5.2531e-05, 1.2341e-04, 2.8526e-06, 2.5000e-05, 2.5304e-06, 1.2045e-04, 6.5686e-05, 2.5448e-06, 3.0747e-05, 6.8505e-04, 1.6349e-04, 1.7177e-05, 5.3074e-05, 2.4046e-04,\n",
       "         6.5379e-05, 2.0955e-04, 6.6566e-06, 2.4964e-04, 1.3157e-04, 4.5472e-05, 1.6776e-05, 3.9367e-04, 2.1900e-04, 3.4126e-05, 5.0587e-05, 2.3584e-05, 5.1437e-04, 8.3990e-05, 5.8314e-05, 4.2039e-05, 3.9190e-05, 2.1091e-05, 1.2904e-05, 1.4680e-05, 7.2132e-05, 7.2801e-05, 1.1880e-05, 9.1417e-05, 2.1487e-05, 1.6274e-05,\n",
       "         3.4266e-06, 3.3721e-05, 8.7047e-05, 3.8422e-06, 1.4974e-05, 7.6765e-06, 8.3685e-06, 3.2052e-05, 8.2632e-05, 5.4281e-05, 1.4336e-05, 4.8338e-05, 3.4581e-05, 9.5243e-06, 2.9073e-05, 1.0779e-04, 6.1516e-06, 1.9308e-05, 6.4350e-05, 5.5180e-05, 6.2357e-06, 4.2018e-06, 8.9283e-06, 1.2771e-04, 2.2820e-04, 1.3272e-03,\n",
       "         1.1349e-04, 3.4268e-05, 9.3682e-06, 1.1873e-04, 9.5452e-05, 3.0836e-04, 1.0217e-05, 1.6371e-05, 6.3383e-06, 1.2521e-04, 1.5380e-04, 2.2651e-04, 1.6943e-04, 3.7406e-04, 5.2311e-04, 5.6970e-04, 4.5382e-04, 5.8423e-04, 7.0075e-05, 1.3290e-04, 2.5084e-04, 6.2666e-05, 2.8788e-04, 2.9989e-02, 8.0004e-05, 3.5873e-04,\n",
       "         4.5500e-05, 7.4151e-05, 3.5161e-05, 8.9827e-06, 3.2741e-05, 1.8797e-04, 4.1192e-06, 5.3782e-05, 3.6647e-05, 2.5856e-05, 2.1429e-04, 2.1976e-04, 2.2468e-04, 6.6990e-05, 3.5482e-05, 1.1240e-04, 7.4608e-04, 7.1234e-05, 1.4126e-04, 1.0282e-04, 2.8058e-05, 5.7797e-04, 2.0847e-04, 1.2252e-05, 1.1255e-02, 2.6898e-04,\n",
       "         5.3443e-04, 1.8194e-04, 2.8680e-03, 2.6012e-04, 2.6076e-04, 1.7764e-05, 7.9879e-03, 1.0435e-04, 5.5579e-05, 3.4241e-06, 2.5859e-05, 1.2472e-05, 1.8476e-05, 8.7301e-06, 2.2023e-05, 1.4318e-04, 8.7796e-05, 1.5311e-04, 2.1932e-04, 7.7801e-04, 3.2577e-02, 9.8706e-05, 3.4106e-05, 4.0637e-05, 5.0675e-06, 5.1781e-06,\n",
       "         2.9042e-05, 7.8969e-05, 3.2613e-06, 3.0716e-04, 2.7505e-05, 1.7881e-05, 3.9328e-06, 2.1975e-04, 4.0441e-05, 4.4844e-05, 5.5082e-06, 4.0514e-06, 4.4848e-05, 8.1190e-04, 4.0338e-04, 1.9520e-04, 7.1045e-05, 6.3763e-07, 3.6300e-04, 1.4394e-04, 2.9930e-04, 1.0046e-04, 6.1360e-06, 2.4784e-04, 6.2565e-04, 6.7868e-05,\n",
       "         3.3840e-06, 5.1078e-05, 1.3068e-05, 9.9188e-06, 3.8560e-06, 5.4605e-06, 4.7775e-06, 2.6134e-04, 3.7779e-04, 2.6984e-04, 3.7204e-05, 5.7910e-06, 8.7246e-05, 3.6431e-05, 2.1553e-05, 2.5438e-06, 5.6058e-05, 1.5782e-06, 9.8803e-06, 7.2490e-06, 1.6126e-05, 1.3040e-04, 2.3179e-05, 1.2580e-06, 2.2149e-06, 2.5373e-05,\n",
       "         1.0116e-04, 8.0589e-05, 3.4805e-05, 2.8831e-05, 2.2993e-05, 9.0211e-05, 6.9413e-06, 6.0258e-07, 3.3094e-05, 1.9615e-03, 1.2795e-05, 2.4652e-05, 1.3213e-04, 3.9393e-06, 2.5444e-03, 1.3737e-03, 4.5277e-06, 1.8452e-05, 1.7644e-06, 6.4055e-04, 9.1978e-07, 9.3040e-06, 3.1319e-05, 6.3611e-05, 2.2954e-06, 1.1293e-06,\n",
       "         7.1466e-05, 1.9030e-02, 1.6185e-04, 7.5688e-03, 3.2868e-05, 1.9792e-05, 6.3142e-05, 3.1451e-06, 6.9303e-06, 2.0937e-04, 1.1638e-04, 2.2782e-03, 4.0184e-05, 4.0850e-04, 7.5193e-05, 8.9348e-06, 3.5447e-04, 1.0203e-05, 1.7201e-05, 3.7658e-04, 2.6768e-05, 1.0396e-04, 5.6912e-04, 1.1128e-05, 1.7473e-05, 4.4759e-04,\n",
       "         3.7405e-05, 3.3226e-06, 4.7557e-06, 1.6556e-05, 8.6119e-04, 3.9820e-05, 3.9804e-05, 3.4796e-05, 2.0740e-06, 1.3384e-04, 1.1909e-05, 3.4600e-06, 2.3212e-06, 8.7077e-06, 2.0225e-04, 4.5322e-05, 2.0103e-05, 2.7158e-04, 1.6762e-05, 1.2112e-03, 5.6172e-04, 2.0185e-04, 4.7483e-06, 5.0426e-05, 5.8148e-06, 4.2804e-05,\n",
       "         4.5216e-05, 3.4985e-03, 7.9587e-04, 2.1802e-04, 1.3350e-05, 5.1969e-06, 4.7293e-06, 7.2004e-05, 2.8368e-06, 4.3065e-06, 2.4267e-04, 1.7011e-05, 2.5684e-04, 6.1461e-05, 1.5598e-05, 2.9700e-05, 4.7492e-05, 3.5693e-05, 3.3416e-04, 1.6477e-04, 5.7077e-05, 2.7410e-05, 1.7818e-05, 1.2444e-04, 2.2505e-05, 1.6931e-05,\n",
       "         4.7159e-03, 1.4748e-06, 1.8306e-06, 1.5675e-05, 3.5007e-06, 1.9777e-03, 8.5405e-05, 3.6825e-05, 2.4201e-05, 6.0164e-05, 9.1896e-05, 5.7969e-05, 5.2228e-05, 4.5656e-05, 2.7531e-05, 3.0895e-05, 8.6600e-05, 1.1348e-05, 2.9301e-06, 3.0652e-05, 1.1136e-05, 2.0502e-04, 9.3917e-07, 3.0401e-04, 2.0301e-05, 7.6522e-04,\n",
       "         1.7625e-06, 1.0684e-03, 7.9269e-05, 1.9027e-05, 2.8891e-04, 2.1322e-05, 9.1838e-06, 2.4167e-06, 1.3509e-06, 1.1518e-05, 1.8860e-04, 2.2602e-05, 3.4343e-05, 4.2936e-05, 1.4410e-03, 5.9873e-07, 1.6403e-05, 1.9571e-06, 1.1970e-04, 2.8069e-04, 1.9825e-03, 6.4455e-06, 2.3714e-04, 5.4245e-05, 3.7536e-04, 1.0180e-03,\n",
       "         2.9047e-06, 1.0351e-06, 3.3738e-06, 1.0621e-03, 8.3757e-07, 2.5735e-03, 2.1333e-03, 1.6119e-04, 2.8413e-03, 2.0013e-04, 4.2697e-03, 2.3539e-04, 4.7899e-05, 5.2704e-05, 8.3976e-06, 2.6348e-06, 6.3198e-04, 1.0170e-05, 9.5934e-07, 1.8221e-05, 2.7584e-06, 2.0752e-03, 1.5984e-05, 1.5015e-05, 1.8061e-05, 3.3392e-06,\n",
       "         1.9018e-05, 8.0544e-07, 1.7822e-03, 3.3097e-06, 1.8263e-06, 1.0273e-02, 2.5920e-05, 1.0799e-05, 2.2338e-05, 3.9022e-06, 1.3201e-05, 8.6902e-05, 1.1325e-04, 7.1671e-04, 9.3505e-06, 2.9932e-05, 3.7562e-05, 2.4088e-05, 2.3991e-05, 6.2334e-05, 3.9363e-05, 1.9648e-04, 1.5792e-05, 5.4797e-05, 2.2886e-04, 3.6752e-06,\n",
       "         1.6458e-05, 2.1823e-04, 1.9170e-03, 6.0484e-04, 1.1814e-04, 6.2251e-06, 2.1604e-04, 2.7691e-05, 7.0043e-05, 4.6210e-06, 8.7880e-06, 2.0187e-05, 2.6583e-05, 9.2852e-05, 1.8209e-06, 7.6046e-06, 6.4312e-06, 8.6776e-05, 3.2190e-04, 1.9555e-04, 2.1036e-04, 1.1002e-03, 1.7718e-03, 1.1796e-05, 1.2886e-03, 4.9113e-05,\n",
       "         2.5369e-06, 5.7081e-05, 2.5279e-04, 3.8088e-05, 1.2505e-05, 9.8761e-04, 6.9139e-06, 1.0317e-04, 2.4917e-04, 8.7505e-05, 2.7778e-05, 1.2529e-05, 6.2893e-05, 3.3174e-05, 3.6291e-06, 6.8350e-06, 1.5008e-04, 2.8268e-05, 5.0371e-06, 5.9993e-05, 2.3144e-03, 3.6493e-05, 1.4623e-04, 6.1898e-04, 6.9900e-06, 3.7594e-05,\n",
       "         6.4490e-04, 1.2902e-04, 8.7716e-06, 2.4638e-03, 6.8791e-06, 4.3133e-04, 5.2007e-05, 2.3120e-02, 1.2934e-04, 1.2433e-04, 8.2693e-05, 5.1652e-07, 1.6290e-04, 1.0970e-05, 1.3749e-04, 4.1226e-06, 7.8748e-05, 1.1317e-05, 2.4063e-06, 2.2353e-04, 1.2556e-05, 4.1810e-05, 5.6575e-05, 7.1651e-05, 6.4349e-04, 4.4026e-06,\n",
       "         1.0538e-05, 6.9626e-03, 2.5707e-04, 4.9883e-05, 2.7740e-04, 3.5671e-04, 4.2990e-05, 5.1573e-04, 8.3124e-05, 6.5214e-05, 1.3446e-04, 2.4420e-07, 3.3213e-04, 7.4986e-05, 1.1319e-06, 4.8043e-05, 1.1180e-04, 3.5270e-05, 1.0049e-03, 6.5921e-05, 2.6323e-04, 8.2213e-06, 5.0206e-06, 2.1163e-04, 4.9304e-03, 2.1346e-02,\n",
       "         4.3380e-06, 2.9908e-06, 5.0085e-05, 3.1636e-05, 2.5886e-05, 4.6622e-05, 4.0743e-05, 7.2936e-06, 1.1048e-05, 6.3573e-05, 1.2611e-03, 3.4587e-05, 2.0661e-03, 4.6277e-05, 5.2210e-05, 1.5827e-05, 2.1280e-05, 2.3594e-04, 6.8350e-05, 1.9736e-05, 3.8092e-03, 2.3479e-05, 2.3578e-05, 1.4048e-04, 5.5990e-06, 9.3084e-05,\n",
       "         6.3220e-04, 1.8399e-05, 1.5640e-05, 2.0193e-03, 2.6990e-06, 3.9801e-04, 1.6553e-06, 4.3902e-05, 1.1366e-05, 1.5367e-04, 6.1285e-04, 1.9017e-05, 5.8528e-04, 3.1205e-04, 3.7807e-05, 2.8467e-05, 1.3840e-02, 5.7445e-04, 9.5075e-04, 1.2001e-03, 1.9009e-06, 3.1010e-03, 7.0579e-05, 8.0128e-05, 2.3249e-05, 2.2836e-04,\n",
       "         1.0307e-05, 7.4647e-04, 2.8832e-05, 1.9428e-06, 1.2131e-04, 1.8264e-05, 2.3674e-04, 1.4099e-04, 1.6847e-07, 1.7327e-04, 2.9761e-04, 2.6332e-06, 5.9690e-04, 1.9390e-04, 7.1824e-05, 1.3120e-03, 8.0417e-06, 1.7722e-05, 1.8650e-02, 1.0642e-04, 3.1514e-06, 2.7032e-04, 1.4306e-04, 1.8464e-05, 3.8193e-05, 1.1177e-05,\n",
       "         2.4750e-05, 1.4822e-05, 6.2148e-04, 5.7531e-04, 2.1837e-03, 1.4174e-05, 1.9200e-05, 4.5106e-04, 1.7639e-06, 3.5869e-04, 4.9805e-06, 3.9143e-05, 6.9383e-05, 5.9290e-05, 1.1285e-03, 3.1551e-04, 7.7596e-05, 7.1054e-05, 2.1619e-04, 1.1172e-05, 1.4509e-07, 8.2411e-06, 3.4995e-04, 1.5225e-04, 3.7589e-05, 1.0779e-04,\n",
       "         5.4900e-05, 1.1479e-04, 3.6379e-05, 1.6632e-05, 2.6176e-05, 3.1001e-04, 1.1110e-02, 1.8996e-03, 1.5516e-05, 4.5863e-04, 1.4143e-04, 1.3155e-06, 2.6435e-03, 4.8455e-04, 1.3410e-04, 4.3674e-05, 7.2391e-05, 2.6436e-05, 1.8874e-04, 7.4019e-06, 2.0436e-04, 2.6106e-04, 1.9283e-04, 2.8113e-05, 2.5488e-05, 1.5706e-05,\n",
       "         2.5055e-05, 1.7534e-05, 2.6864e-04, 5.8101e-06, 9.4586e-05, 5.1474e-04, 1.2889e-04, 4.4530e-06, 7.8100e-05, 8.8889e-05, 1.3359e-04, 2.6953e-05, 4.5263e-04, 6.4166e-04, 1.7997e-04, 9.4676e-05, 3.3541e-06, 1.2110e-05, 5.8353e-05, 4.7777e-04, 6.8059e-04, 1.0605e-06, 1.9603e-03, 1.3968e-03, 1.5557e-04, 5.2428e-06,\n",
       "         2.3762e-05, 2.0969e-04, 2.9569e-05, 2.1348e-04, 1.3755e-04, 1.9216e-06, 1.0875e-06, 2.7987e-05, 2.6261e-05, 1.3470e-04, 6.0628e-06, 1.3825e-05, 9.1955e-05, 9.5110e-05, 5.0431e-04, 2.8622e-06, 2.4947e-06, 8.5952e-05, 8.7832e-04, 1.1031e-05, 1.0546e-06, 2.5087e-04, 2.1738e-04, 8.4581e-06, 6.0956e-05, 3.3847e-04,\n",
       "         5.5757e-05, 6.9042e-03, 1.8987e-06, 2.0154e-06, 1.9866e-04, 3.2948e-04, 1.7310e-06, 5.1153e-05, 8.6509e-04, 7.6766e-05, 7.1074e-05, 6.8379e-03, 2.8038e-05, 1.8378e-05, 1.1859e-05, 9.2024e-05, 2.3510e-04, 3.0163e-04, 2.7326e-04, 4.8236e-05, 2.1683e-03, 1.0165e-04, 1.5969e-04, 4.4468e-05, 5.6928e-03, 8.2439e-03,\n",
       "         1.0487e-04, 1.0550e-02, 1.1836e-04, 2.7233e-04, 2.0226e-05, 2.1537e-06, 4.9004e-05, 1.1794e-04, 7.6851e-06, 1.1700e-04, 5.0670e-04, 3.0836e-04, 8.9464e-05, 2.6438e-05, 2.7174e-07, 1.8102e-03, 1.9230e-03, 2.1175e-05, 8.3976e-06, 3.4921e-05, 2.3254e-04, 2.5614e-05, 5.9302e-06, 1.7440e-06, 4.0167e-06, 8.8511e-05,\n",
       "         2.5226e-05, 9.3441e-06, 1.1827e-04, 6.5510e-07, 1.5682e-04, 3.9738e-05, 9.9246e-05, 1.0401e-05, 1.3513e-06, 1.7737e-05, 1.2759e-04, 9.7520e-05, 3.9547e-04, 5.5365e-05, 5.1600e-05, 2.4324e-05, 2.4721e-04, 3.4401e-06, 1.3465e-05, 7.6857e-07, 1.0245e-05, 9.0750e-05, 3.0446e-04, 7.5311e-05, 4.8099e-06, 4.1038e-05,\n",
       "         3.3987e-05, 1.2556e-05, 3.5390e-05, 8.0758e-07, 1.7117e-04, 1.9331e-05, 1.0209e-04, 2.2458e-04, 4.2402e-04, 4.0446e-03, 1.4505e-04, 4.9451e-05, 2.5189e-02, 1.3268e-04, 2.2169e-04, 3.1586e-04, 2.4609e-04, 1.5461e-04, 1.9704e-01, 5.1790e-05, 6.9767e-06, 5.8383e-05, 1.8550e-05, 1.1145e-04, 1.7822e-06, 1.6566e-04,\n",
       "         2.7272e-05, 7.0668e-04, 7.6051e-06, 4.3576e-05, 1.2095e-05, 7.1763e-05, 2.1462e-05, 3.5648e-04, 9.9436e-05, 4.2667e-05, 3.5181e-04, 8.7105e-05]], device='cuda:0')"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.onnx\n",
    "\n",
    "\n",
    "# from your_model_file import YourModelClass  # Import your model class\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Step 1: Load the model\n",
    "model = YoloSkinLesionModel().to(device) # Instantiate your model class\n",
    "\n",
    "# model.load_state_dict(torch.load('model/yolo_skin.pth'))  # Load the state dict\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "img = f'../dataset/masked_images/ISIC_0159874.jpg'  # or a video file\n",
    "image = Image.open(img).convert('RGB')\n",
    "image = image.resize((512, 512))\n",
    "image = np.array(image, dtype=np.float32)\n",
    "image = image * (1 / 255.0)\n",
    "image = np.transpose(image, (2, 0, 1))\n",
    "image = torch.from_numpy(image).to(device)\n",
    "image = image.unsqueeze(0)\n",
    "result = model(image.to(device))\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75feb231",
   "metadata": {},
   "source": [
    "### tricorder-3 custom model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "4e99302c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "yolo_model = YOLO(\"yolo11n-cls\")\n",
    "\n",
    "# Define the model class (ensure this matches the saved model architecture)\n",
    "class SimpleSkinLesionModel(nn.Module):\n",
    "    def __init__(self, num_classes=11, num_demographics=3):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            *list(yolo_model.model.model)[:-1]  # Use all layers except the final classification layer\n",
    "        )\n",
    "        last_layer = list(yolo_model.model.model)[-1]\n",
    "        self.last_conv = last_layer.conv\n",
    "        self.last_pool = last_layer.pool\n",
    "        self.list_drop = last_layer.drop\n",
    "        \n",
    "        \n",
    "        self.demographics_processor = nn.Sequential(\n",
    "            nn.Linear(num_demographics, 16),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Linear(1280 + 16 , num_classes, bias=True)\n",
    "\n",
    "    def forward(self, image, demographics):\n",
    "        image_features = self.features(image)\n",
    "        image_features = self.last_conv(image_features)\n",
    "        image_features = self.last_pool(image_features)\n",
    "        image_features = self.list_drop(image_features)\n",
    "        image_features = image_features.view(image_features.size(0), -1)    \n",
    "    \n",
    "        demographics_features = self.demographics_processor(demographics)\n",
    "        combined_features = torch.cat((image_features, demographics_features), dim=1)\n",
    "        logits = self.classifier(combined_features)\n",
    "        # probabilities = nn.functional.softmax(logits, dim=1)\n",
    "        return logits\n",
    "\n",
    "\n",
    "####\n",
    "\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from torchvision.models import efficientnet_b7, EfficientNet_B7_Weights\n",
    "\n",
    "# class AccurateSkinLesionModel(nn.Module):\n",
    "#     def __init__(self, num_classes=10, num_demographics=3):\n",
    "#         super().__init__()\n",
    "#         # Load pre-trained EfficientNet-B7 for high accuracy\n",
    "#         backbone = efficientnet_b7(weights=EfficientNet_B7_Weights.IMAGENET1K_V1)\n",
    "#         # Extract features up to the adaptive average pooling (exclude the final classifier)\n",
    "#         self.features = nn.Sequential(*list(backbone.children())[:-2])  # Up to features, before avgpool and classifier\n",
    "        \n",
    "#         # Global average pooling to get fixed-size features\n",
    "#         self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "#         self.demographics_processor = nn.Sequential(\n",
    "#             nn.Linear(num_demographics, 64),\n",
    "#             nn.BatchNorm1d(64),\n",
    "#             nn.ReLU(inplace=True)\n",
    "#         )\n",
    "\n",
    "#         # EfficientNet-B7 outputs 2560 features after pooling\n",
    "#         self.classifier = nn.Linear(2560 + 64, num_classes, bias=False)\n",
    "\n",
    "#     def forward(self, image, demographics):\n",
    "#         # Extract image features (output shape: (B, 2560, 7, 7) or similar, then pool)\n",
    "#         image_features = self.features(image)\n",
    "#         image_features = self.avgpool(image_features)\n",
    "#         image_features = torch.flatten(image_features, start_dim=1)\n",
    "        \n",
    "#         # Process demographics\n",
    "#         demographics_features = self.demographics_processor(demographics)\n",
    "        \n",
    "#         # Combine and classify\n",
    "#         combined_features = torch.cat((image_features, demographics_features), dim=1)\n",
    "#         logits = self.classifier(combined_features)\n",
    "        \n",
    "#         return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "a445acd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Class\n",
       "1     2518\n",
       "8      712\n",
       "3      541\n",
       "9      473\n",
       "7      444\n",
       "0      303\n",
       "4       52\n",
       "5       49\n",
       "10      45\n",
       "2       43\n",
       "6        9\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "1ebd20d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ultralytics\n",
    "# Main training loop\n",
    "def main():\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(device)\n",
    "    # Initialize the model, loss function, and optimizer\n",
    "    model = SimpleSkinLesionModel(num_classes=11, num_demographics=3).to(device)\n",
    "    # model.load_state_dict(torch.load('skin_lesion_model_2_2.pth'), strict = False)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=3*1e-4)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=1/3)\n",
    "    # Prepare the datasets and dataloaders\n",
    "    train_dataset = SkinLesionDataset(data)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "    print(train_loader)\n",
    "    \n",
    "    \n",
    "    val_dataset = SkinLesionDataset(data)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False)\n",
    "\n",
    "    # Train and validate the model\n",
    "    train_model(model, train_loader, criterion, optimizer, scheduler, num_epochs=300, device=device)\n",
    "    validate_model(model, val_loader, device=device)\n",
    "\n",
    "    # Save the model\n",
    "    torch.save(model.state_dict(), 'skin_lesion_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988e73fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[183], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTORCH_USE_CUDA_DSA\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m----> 5\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[181], line 8\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(device)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Initialize the model, loss function, and optimizer\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSimpleSkinLesionModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m11\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_demographics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# model.load_state_dict(torch.load('skin_lesion_model_2_2.pth'), strict = False)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n",
      "File \u001b[0;32m/home/mateo/cancer-ai/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1355\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1352\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1353\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1355\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/mateo/cancer-ai/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:915\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 915\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    918\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    919\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    920\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    925\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    926\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/home/mateo/cancer-ai/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:915\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 915\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    918\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    919\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    920\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    925\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    926\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/home/mateo/cancer-ai/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:915\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 915\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    918\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    919\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    920\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    925\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    926\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/home/mateo/cancer-ai/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:942\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    938\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    939\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    940\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 942\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    943\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    945\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m/home/mateo/cancer-ai/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1341\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1334\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1335\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1336\u001b[0m             device,\n\u001b[1;32m   1337\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1338\u001b[0m             non_blocking,\n\u001b[1;32m   1339\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1340\u001b[0m         )\n\u001b[0;32m-> 1341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1342\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1343\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1344\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1345\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1347\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fb38ca",
   "metadata": {},
   "source": [
    "### Save onnx file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f003072",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.onnx\n",
    "# from your_model_file import YourModelClass  # Import your model class\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Step 1: Load the model\n",
    "model = YoloSkinLesionModel() # Instantiate your model class\n",
    "\n",
    "# model.load_state_dict(torch.load('model/yolo_skin.pth'))  # Load the state dict\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Step 2: Create a dummy input\n",
    "dummy_image = torch.randn(1, 3, 512, 512)  # Adjust the shape as needed\n",
    "dummy_demo = torch.tensor([[42.0, 1.0, 5.0]])  # age, sex, location\n",
    "# Step 3: Export the model to ONNX\n",
    "torch.onnx.export(\n",
    "    model.to(device),\n",
    "    (dummy_image.to(device)),\n",
    "    \"model/yolo_skin.onnx\",\n",
    "    export_params=True,\n",
    "    opset_version=13,\n",
    "    do_constant_folding=True,\n",
    "    input_names=['image'],\n",
    "    output_names=['output'],\n",
    "    dynamic_axes={\n",
    "        'image': {0: 'batch_size'},\n",
    "        'output': {0: 'batch_size'}\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9845b723",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "from PIL import Image\n",
    "import torch\n",
    "model_state = torch.load(\"./skin_lesion_model.pth\")\n",
    "model = SimpleSkinLesionModel().to(device)\n",
    "model.load_state_dict(model_state, strict = False)\n",
    "model.eval()\n",
    "image = Image.open(f\"example_dataset/dataset00092/a954cebc-6d49-4750-b485-851a307ab3fb.jpg\").convert('RGB')\n",
    "image = image.resize((512, 512))\n",
    "image = np.array(image, dtype=np.float32)\n",
    "image = image * (512.0 / 255.0)\n",
    "\n",
    "image = np.transpose(image, (2, 0, 1))\n",
    "image = torch.from_numpy(image).to(device)\n",
    "image = image.unsqueeze(0)\n",
    "# print(image.shape)\n",
    "data = torch.tensor([31, 0 ,7] , dtype = torch.float32).unsqueeze(0).to(device)\n",
    "print(data.shape)\n",
    "print(image.shape)\n",
    "result = model(image, data)\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165c06b1",
   "metadata": {},
   "source": [
    "### sharren/vit-beta2-0.9995 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42c1aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import ViTForImageClassification, ViTConfig\n",
    "\n",
    "# Load the configuration\n",
    "config = ViTConfig.from_pretrained(\"config.json\")\n",
    "\n",
    "# Load the model\n",
    "model = ViTForImageClassification.from_pretrained(\"model.safetensors\", config=config)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280a0660",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Define the config_imguration\n",
    "config_img = {\n",
    "    \"do_normalize\": True,\n",
    "    \"do_rescale\": True,\n",
    "    \"do_resize\": True,\n",
    "    \"image_mean\": [0.5, 0.5, 0.5],\n",
    "    \"image_std\": [0.5, 0.5, 0.5],\n",
    "    \"size\": {\"height\": 224, \"width\": 224},\n",
    "    \"rescale_factor\": 0.00392156862745098,\n",
    "    \"resample\": 2  # Bilinear\n",
    "}\n",
    "\n",
    "# Define the preprocessing transformations\n",
    "transformations = []\n",
    "\n",
    "if config_img[\"do_resize\"]:\n",
    "    transformations.append(transforms.Resize((config_img[\"size\"][\"height\"], config_img[\"size\"][\"width\"])))\n",
    "transformations.append(transforms.ToTensor())\n",
    "if config_img[\"do_rescale\"]:\n",
    "    transformations.append(transforms.Lambda(lambda x: x * config_img[\"rescale_factor\"]))\n",
    "\n",
    "if config_img[\"do_normalize\"]:\n",
    "    transformations.append(transforms.Normalize(mean=config_img[\"image_mean\"], std=config_img[\"image_std\"]))\n",
    "\n",
    "# Create a composed transformation\n",
    "preprocess = transforms.Compose(transformations)\n",
    "\n",
    "# Load and preprocess the image\n",
    "image_path = \"example_dataset/dataset00092/a954cebc-6d49-4750-b485-851a307ab3fb.jpg\"  # Change this to your image path\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "# image = np.array(image, dtype = np.float32)\n",
    "# Apply the transformations\n",
    "input_tensor = preprocess(image)\n",
    "input_batch = input_tensor.unsqueeze(0)  # Create a mini-batch as expected by the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb14be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():  # Disable gradient calculation\n",
    "outputs = model(input_batch)\n",
    "\n",
    "\n",
    "print(outputs)\n",
    "# Get the predicted class\n",
    "predicted_class = torch.argmax(outputs.logits, dim=1).item()\n",
    "print(predicted_class)\n",
    "# Map the predicted class index to the label\n",
    "id2label = config.id2label\n",
    "predicted_label = id2label[predicted_class]\n",
    "\n",
    "print(f'Predicted class: {predicted_label}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1887e308",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d94766ec",
   "metadata": {},
   "source": [
    "### Resnet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910df3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet18(pretrained=True)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46a4f20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".cancer (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
